{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fa3a08-111f-4d26-8e65-7be9ba004054",
   "metadata": {},
   "source": [
    "# Model Implementation for 3D Cell Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd6961-5904-42c4-9fd7-190c0685c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary \n",
    "# !pip install gunpowder\n",
    "# !pip install zarr\n",
    "# !pip install matplotlib\n",
    "# pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503c9e5-eea9-4751-9600-fd8c537aecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yserentantk/miniconda3/envs/celltracking/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gunpowder as gp\n",
    "import zarr\n",
    "import math\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import skimage\n",
    "import networkx\n",
    "import pathlib\n",
    "from tifffile import imread, imwrite\n",
    "import tensorboard\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c90df-e975-4373-9a30-ff0977d23544",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c18c9-a9f7-4312-8b09-501b6bbc53d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93a13da-0440-4782-a888-a49085a172d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert 'TRA' channel into cell and frame-wise centroid positions\n",
    "## Function to extract trajectories from data\n",
    "\n",
    "base_path = pathlib.Path(\"/mnt/shared/celltracking/data/cho/\")\n",
    "\n",
    "\n",
    "# read parent-child links from file\n",
    "links = np.loadtxt(base_path / \"01_GT/TRA\" / \"man_track.txt\", dtype=int)\n",
    "\n",
    "# read annotated image stack\n",
    "centroids = np.stack([imread(xi) for xi in sorted((base_path / \"01_GT/TRA\").glob(\"*.tif\"))])  # images\n",
    "\n",
    "# extract centroids from annotated image stacks\n",
    "centers = skimage.measure.regionprops(centroids[0,0,:,:])\n",
    "tracks = []\n",
    "for t, frame in enumerate(centroids):\n",
    "    centers = skimage.measure.regionprops(frame)\n",
    "    for c in centers:\n",
    "        tracks.append([c.label, t, int(c.centroid[1]), int(c.centroid[2])])\n",
    "        \n",
    "# constructs graph \n",
    "tracks = np.array(tracks)\n",
    "graph = networkx.DiGraph()\n",
    "for cell_id, t, x, y in tracks:\n",
    "    graph.add_node((cell_id,t), x=x, y=y, t=t)\n",
    "    \n",
    "for cell_id, t in graph.nodes():\n",
    "    if (cell_id, t+1) in graph.nodes():\n",
    "        graph.add_edge((cell_id, t), (cell_id,t+1))\n",
    "\n",
    "for child_id, child_from, _, child_parent_id in links:\n",
    "    for parent_id, _, parent_to, _ in links:\n",
    "        if child_parent_id == parent_id:\n",
    "            graph.add_edge((parent_id, parent_to), (child_id, child_from))\n",
    "            \n",
    "# extract trajectories from graph set\n",
    "tracks = [graph.subgraph(c) for c in networkx.weakly_connected_components(graph) if len(c)>0]\n",
    "\n",
    "# remove tracks with 0 edges\n",
    "tracks = [track for track in tracks if len(track.edges)>0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226e7f8-90f8-41ee-ad99-f0b63fddd0f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define function to make image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a770c1f1-4e52-49ae-9f50-abd61b92b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getPaired(gp.BatchFilter):\n",
    "\n",
    "    def __init__(self, raw, raw_shift, tracks, paired=True):\n",
    "        self.raw = raw\n",
    "        self.raw_shift = raw_shift\n",
    "        self.tracks = tracks\n",
    "        self.paired = paired\n",
    "    \n",
    "    # _ref channel array is stored in raw_ref, while second volume in pair will be stored raw_new\n",
    "    def prepare(self, request):\n",
    "        # obtain volume coordinates from tracks                \n",
    "        deps = gp.BatchRequest()\n",
    "        vol1,vol2 = self.sampler(request)\n",
    "                \n",
    "        deps[self.raw] = gp.ArraySpec(roi=gp.Roi(vol1,request[self.raw].roi.get_shape()))\n",
    "        deps[self.raw_shift] = gp.ArraySpec(roi=gp.Roi(vol2,request[self.raw_shift].roi.get_shape()))\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    # required to inform downstream nodes about new array \n",
    "    def process(self, batch, request):\n",
    "        # create a new batch to hold the new array\n",
    "        out_batch = gp.Batch()\n",
    "\n",
    "        # create new array and store it in the batch\n",
    "        out_batch[self.raw_shift] = batch[self.raw_shift]\n",
    "        out_batch[self.raw] = batch[self.raw]\n",
    "        \n",
    "        #print(f'raw: {batch[self.raw].spec.roi}')\n",
    "        #print(batch[self.raw_shift].spec.roi)\n",
    "        \n",
    "        # make sure that coordinates for batch[raw] and batch[raw_shift] are reset to (0,0,0,0,0)\n",
    "        out_batch[self.raw].spec.roi = request[self.raw].roi\n",
    "        out_batch[self.raw_shift].spec.roi = request[self.raw_shift].roi\n",
    "\n",
    "        # return the new batch\n",
    "        return out_batch\n",
    "    \n",
    "    # select pairs of subvolumes from data\n",
    "    def sampler(self,request):\n",
    "        tracks = self.tracks\n",
    "        paired = self.paired\n",
    "        # choose connected nodes\n",
    "        # if self.paired:\n",
    "        if paired:\n",
    "            t0 = tracks[np.random.randint(0,len(tracks),1).item()]\n",
    "            e0 = list(t0.edges)[np.random.randint(len(list(t0.edges)))]\n",
    "            node0 = t0.nodes[e0[0]]\n",
    "            node1 = t0.nodes[e0[1]]\n",
    "            \n",
    "        # choose random unconnected nodes\n",
    "        else:\n",
    "            # randomly choose two tracks and make sure they are not identical\n",
    "            t0,t1 = np.random.randint(0,len(tracks),2)\n",
    "            while t0==t1:\n",
    "                t0,t1 = np.random.randint(0,len(tracks),2)\n",
    "\n",
    "            #print(f'trackids: {t0,t1}')\n",
    "            t0 = tracks[t0]\n",
    "            t1 = tracks[t1]\n",
    "\n",
    "            # choose random edges from each track\n",
    "            #print(f'number edges per track{len(list(t0.nodes)),len(list(t1.nodes))}')\n",
    "\n",
    "            r0 = np.random.randint(0,len(list(t0.nodes))) \n",
    "            r1 = np.random.randint(0,len(list(t1.nodes)))\n",
    "\n",
    "            node0 = t0.nodes[list(t0.nodes)[r0]]\n",
    "            node1 = t1.nodes[list(t1.nodes)[r1]]\n",
    "            \n",
    "\n",
    "\n",
    "        node0_xyt = [node0[\"x\"], node0[\"y\"], node0[\"t\"]]\n",
    "        node1_xyt = [node1[\"x\"], node1[\"y\"], node1[\"t\"]]\n",
    "\n",
    "        #print(f'input coord: {node0_xyt,node1_xyt}')\n",
    "\n",
    "        roi_in = request[self.raw_shift].roi.get_shape()\n",
    "        #t,z,y,x\n",
    "        coords_vol0 = (node0_xyt[2],0,node0_xyt[0]-(roi_in[2]/2),node0_xyt[1]-(roi_in[3]/2))\n",
    "        coords_vol1 = (node1_xyt[2],0,node1_xyt[0]-(roi_in[2]/2),node1_xyt[1]-(roi_in[3]/2))\n",
    "        #print(f'output coords - vol0: {coords_vol0}, vol1:{coords_vol1}')\n",
    "\n",
    "        return coords_vol0, coords_vol1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe16d12-0fe2-41dd-bf22-eeaf290b3e65",
   "metadata": {},
   "source": [
    "# Make batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9374eed7-3581-4bae-a4e3-a59c6134da27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROI: None, voxel size: None, interpolatable: None, non-spatial: False, dtype: None, placeholder: False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify subvolume size and volume source\n",
    "volSize = (1,5,64, 64)\n",
    "coord = (0,0,0,0)\n",
    "batch_size = 8\n",
    "\n",
    "zarrdir = '/mnt/shared/celltracking/data/cho/01.zarr'\n",
    "raw = gp.ArrayKey('raw')\n",
    "raw_shift = gp.ArrayKey('raw_shift')\n",
    "\n",
    "# chose a random source (i.e., sample) from the above\n",
    "random_location = gp.RandomLocation()\n",
    "pipeline_paired = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "    )+ gp.Normalize(raw)+gp.Normalize(raw_shift)+ \n",
    "    gp.Pad(raw_shift, None) + \n",
    "    gp.Pad(raw, None) + gp.IntensityAugment(\n",
    "    raw,\n",
    "    scale_min=0.9,\n",
    "    scale_max=1.1,\n",
    "    shift_min=-0.1,\n",
    "    shift_max=0.1,\n",
    "    ) + gp.NoiseAugment(raw, mode=\"gaussian\")) + gp.IntensityAugment(\n",
    "    raw_shift,\n",
    "    scale_min=0.9,\n",
    "    scale_max=1.1,\n",
    "    shift_min=-0.1,\n",
    "    shift_max=0.1,\n",
    "    ) + gp.NoiseAugment(raw_shift, mode=\"gaussian\") + getPaired(raw,raw_shift,tracks,paired=True) + gp.ElasticAugment(\n",
    "    [2,10,10],\n",
    "    [0,2,2],\n",
    "    [0,0*math.pi/2.0],\n",
    "    prob_slip=0.05,\n",
    "    prob_shift=0.05,\n",
    "    max_misalign=25) + gp.SimpleAugment(transpose_only=[2, 3], mirror_only=[])\n",
    "\n",
    "pipeline_unpaired = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "    )+ gp.Normalize(raw)+gp.Normalize(raw_shift)+ \n",
    "    gp.Pad(raw_shift, None) + \n",
    "    gp.Pad(raw, None) + gp.IntensityAugment(\n",
    "    raw,\n",
    "    scale_min=0.9,\n",
    "    scale_max=1.1,\n",
    "    shift_min=-0.1,\n",
    "    shift_max=0.1,\n",
    "    ) + gp.NoiseAugment(raw, mode=\"gaussian\") + gp.IntensityAugment(\n",
    "    raw_shift,\n",
    "    scale_min=0.9,\n",
    "    scale_max=1.1,\n",
    "    shift_min=-0.1,\n",
    "    shift_max=0.1,\n",
    "    ) + gp.NoiseAugment(raw_shift, mode=\"gaussian\")) + getPaired(raw,raw_shift,tracks,paired=False)  + gp.ElasticAugment(\n",
    "    [2,10,10],\n",
    "    [0,2,2],\n",
    "    [0,0*math.pi/2.0],\n",
    "    prob_slip=0.05,\n",
    "    prob_shift=0.05,\n",
    "    max_misalign=25) + gp.SimpleAugment(transpose_only=[2, 3], mirror_only=[]) \n",
    "\n",
    "# pipeline_paired = (gp.ZarrSource(\n",
    "#     zarrdir,  # the zarr container\n",
    "#     {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "#     {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "#     ) + gp.Pad(raw_shift, None) + gp.Pad(raw, None) + getPaired(raw,raw_shift,tracks,paired=True))\n",
    "\n",
    "# pipeline_unpaired = (gp.ZarrSource(\n",
    "#     zarrdir,  # the zarr container\n",
    "#     {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "#     {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "#     ) + gp.Pad(raw_shift, None) + gp.Pad(raw, None) + getPaired(raw,raw_shift,tracks,paired=False))\n",
    "\n",
    "\n",
    "pipeline_paired += gp.PreCache(num_workers=6) \n",
    "pipeline_unpaired += gp.PreCache(num_workers=6)\n",
    "pipeline_paired += gp.Stack(batch_size)\n",
    "pipeline_unpaired += gp.Stack(batch_size)\n",
    "\n",
    "# specify request\n",
    "request = gp.BatchRequest()\n",
    "request[raw] = gp.Roi(coord, volSize)\n",
    "request[raw_shift] = gp.Roi(coord, volSize)\n",
    "\n",
    "gp.ArraySpec()\n",
    "# #build the pipeline...\n",
    "# with gp.build(pipeline_paired):\n",
    "\n",
    "#   #...and request a batch\n",
    "#   batch = pipeline_paired.request_batch(request)\n",
    "  \n",
    "# #show the content of the batch\n",
    "# print(f\"batch returned: {batch}\")\n",
    "\n",
    "# #plot first slice of volume\n",
    "# fig, axs = plt.subplots(1,2, figsize=(10,5),dpi=300)\n",
    "# print(batch[raw].data.shape)\n",
    "# axs[0].imshow(np.flipud(batch[raw].data[0,0,0,:,:]))\n",
    "# axs[1].imshow(np.flipud(batch[raw_shift].data[0,0,0,:,:]))\n",
    "# axs[0].set_xticks([])\n",
    "# axs[1].set_xticks([])\n",
    "# axs[0].set_yticks([])\n",
    "# axs[1].set_yticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead44e01-a035-424e-84e3-3d2968cf0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide path to zarr directory\n",
    "zarrdir = '/mnt/shared/celltracking/data/cho/01.zarr'\n",
    "\n",
    "data = zarr.open(zarrdir)\n",
    "loader = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f74b5-d746-4fe4-8661-2f5efcc0e95b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4517e490-95cb-4786-b8d8-52b7a051da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_classes, downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.downsample_factors = downsample_factors\n",
    "        self.output_classes = 2\n",
    "\n",
    "        current_fmaps, h, w, d = tuple(input_size)\n",
    "        current_size = (h, w,d)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1]*current_size[2] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff22bf-d49f-42bf-b92d-b110037d97fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loss Functions\n",
    "\n",
    "We'll probably need to test some different loss functions. List some here:\n",
    "Contrastive loss\n",
    "cosine similarity\n",
    "triplet loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c52a2b5-1749-4209-b022-41a445109799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"Contrastive loss function\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "            + (label)\n",
    "            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da12ed3-cb32-4e26-809d-8aed7cf9ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1        [-1, 12, 64, 64, 5]             336\n",
      "       BatchNorm3d-2        [-1, 12, 64, 64, 5]              24\n",
      "              ReLU-3        [-1, 12, 64, 64, 5]               0\n",
      "            Conv3d-4        [-1, 12, 64, 64, 5]           3,900\n",
      "       BatchNorm3d-5        [-1, 12, 64, 64, 5]              24\n",
      "              ReLU-6        [-1, 12, 64, 64, 5]               0\n",
      "         MaxPool3d-7        [-1, 12, 32, 32, 5]               0\n",
      "            Conv3d-8        [-1, 24, 32, 32, 5]           7,800\n",
      "       BatchNorm3d-9        [-1, 24, 32, 32, 5]              48\n",
      "             ReLU-10        [-1, 24, 32, 32, 5]               0\n",
      "           Conv3d-11        [-1, 24, 32, 32, 5]          15,576\n",
      "      BatchNorm3d-12        [-1, 24, 32, 32, 5]              48\n",
      "             ReLU-13        [-1, 24, 32, 32, 5]               0\n",
      "        MaxPool3d-14        [-1, 24, 16, 16, 5]               0\n",
      "           Conv3d-15        [-1, 48, 16, 16, 5]          31,152\n",
      "      BatchNorm3d-16        [-1, 48, 16, 16, 5]              96\n",
      "             ReLU-17        [-1, 48, 16, 16, 5]               0\n",
      "           Conv3d-18        [-1, 48, 16, 16, 5]          62,256\n",
      "      BatchNorm3d-19        [-1, 48, 16, 16, 5]              96\n",
      "             ReLU-20        [-1, 48, 16, 16, 5]               0\n",
      "        MaxPool3d-21          [-1, 48, 8, 8, 5]               0\n",
      "           Conv3d-22          [-1, 96, 8, 8, 5]         124,512\n",
      "      BatchNorm3d-23          [-1, 96, 8, 8, 5]             192\n",
      "             ReLU-24          [-1, 96, 8, 8, 5]               0\n",
      "           Conv3d-25          [-1, 96, 8, 8, 5]         248,928\n",
      "      BatchNorm3d-26          [-1, 96, 8, 8, 5]             192\n",
      "             ReLU-27          [-1, 96, 8, 8, 5]               0\n",
      "        MaxPool3d-28          [-1, 96, 4, 4, 5]               0\n",
      "           Linear-29                 [-1, 4096]      31,461,376\n",
      "             ReLU-30                 [-1, 4096]               0\n",
      "          Dropout-31                 [-1, 4096]               0\n",
      "           Linear-32                 [-1, 4096]      16,781,312\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "           Linear-35                   [-1, 32]         131,104\n",
      "================================================================\n",
      "Total params: 48,868,972\n",
      "Trainable params: 48,868,972\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 22.16\n",
      "Params size (MB): 186.42\n",
      "Estimated Total Size (MB): 208.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = (1, 64, 64, 5)\n",
    "downsample_factors =[(2, 2, 1), (2, 2, 1), (2, 2, 1), (2, 2, 1)];\n",
    "output_classes = 32\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg3D(input_size, output_classes,  downsample_factors = downsample_factors)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27bbf7d8-1dfb-441b-a523-894c3be93493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training length\n",
    "epochs = 100\n",
    "\n",
    "#loss_function = torch.nn.BCELoss()\n",
    "loss_function = torch.nn.CosineEmbeddingLoss()\n",
    "#loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03dec6c-14c4-4400-8ff5-d523845d6105",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79686e15-7006-4f8d-8875-b2f9aa03b396",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementing the Siamese Network\n",
    "\n",
    "The above training is just to test if the VGG model works for 3D data. Here, the training will take two pairs of images and calculate the loss from both pairs of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a74c3a-e50e-47c1-9689-4cbea55308e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir models\n",
    "logger = SummaryWriter()\n",
    "#%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71160755-6f76-42e1-9d36-c70fca812139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "step:10\n",
      "step:20\n",
      "step:30\n",
      "step:40\n",
      "step:50\n",
      "step:60\n",
      "step:70\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train(tb_logger = None, log_image_interval = 10):\n",
    "    \n",
    "    \n",
    "    ############################## HARD CODED MODEL LOAD\n",
    "    model.load_state_dict(torch.load('/mnt/shared/celltracking/modelstates/rerun_32output_augment/epoch_2'))\n",
    "    ############################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    loss=[] \n",
    "    counter=[]\n",
    "    with gp.build(pipeline_paired), gp.build(pipeline_unpaired):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            epoch_loss = 0\n",
    "            epoch_loss_pos = 0\n",
    "            epoch_loss_neg = 0\n",
    "            for x in range(500):\n",
    "                unpaired = pipeline_unpaired.request_batch(request)\n",
    "                yu = -1 #zero if using contrastive loss, -1 if using cosine similarity\n",
    "                \n",
    "                paired = pipeline_paired.request_batch(request)\n",
    "                yp = 1\n",
    "                \n",
    "                unpaired1 = unpaired[raw].data\n",
    "                unpaired2 = unpaired[raw_shift].data\n",
    "                paired1 = paired[raw].data\n",
    "                paired2 = paired[raw_shift].data\n",
    "                \n",
    "                unpaired1 = np.reshape(unpaired1, (batch_size,64, 64, 5))\n",
    "                unpaired2 = np.reshape(unpaired2, (batch_size,64, 64, 5))\n",
    "                paired1 = np.reshape(paired1, (batch_size,64, 64, 5))\n",
    "                paired2 = np.reshape(paired2, (batch_size,64, 64, 5))\n",
    "                \n",
    "                # unpaired1 = np.reshape(unpaired1, (batch_size,16, 16, 5))\n",
    "                # unpaired2 = np.reshape(unpaired2, (batch_size,16, 16, 5))\n",
    "                # paired1 = np.reshape(paired1, (batch_size,16, 16, 5))\n",
    "                # paired2 = np.reshape(paired2, (batch_size,16, 16, 5))\n",
    "                \n",
    "                unpaired1 = np.expand_dims(unpaired1, axis =1)\n",
    "                unpaired2 = np.expand_dims(unpaired2, axis=1)\n",
    "                paired1 = np.expand_dims(paired1, axis =1)\n",
    "                paired2 = np.expand_dims(paired2, axis=1)\n",
    "\n",
    "                unpaired1 = torch.from_numpy(unpaired1).to(device).float()\n",
    "                unpaired2 = torch.from_numpy(unpaired2).to(device).float()\n",
    "                yu = torch.from_numpy(np.array([yu])).to(device).float()\n",
    "                \n",
    "                paired1 = torch.from_numpy(paired1).to(device).float()\n",
    "                paired2 = torch.from_numpy(paired2).to(device).float() \n",
    "                yp = torch.from_numpy(np.array([yp])).to(device).float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predp1 = model(paired1)\n",
    "                predp2 = model(paired2)\n",
    "                predu1 = model(unpaired1)\n",
    "                predu2 = model(unpaired2)\n",
    "                #print(model(unpaired1).shape)\n",
    "                #print(predp1.shape)\n",
    "\n",
    "                #loss = loss_function(pred, y)\n",
    "                \n",
    "                #print(predp1.shape)\n",
    "                if x%10==0:\n",
    "                    print(f'step:{x}')\n",
    "\n",
    "                loss_contrastivep = loss_function(predp1,predp2,yp)\n",
    "                loss_contrastiveu = loss_function(predu1,predu2,yu)\n",
    "\n",
    "                loss_contrastivep.backward()\n",
    "                loss_contrastiveu.backward()\n",
    "                optimizer.step()    \n",
    "                epoch_loss_pos += loss_contrastivep\n",
    "                epoch_loss_neg += loss_contrastiveu\n",
    "                epoch_loss += loss_contrastivep + loss_contrastiveu\n",
    "                \n",
    "                \n",
    "                \n",
    "                if tb_logger is not None:\n",
    "                    step = epoch * 10 + x\n",
    "                    tb_logger.add_scalar(\n",
    "                        tag=\"positive_loss\", scalar_value=loss_contrastivep.item(), global_step=step\n",
    "                    )\n",
    "                    tb_logger.add_scalar(\n",
    "                        tag=\"negative_loss\", scalar_value=loss_contrastiveu.item(), global_step=step\n",
    "                    )\n",
    "                    tb_logger.add_scalar(\n",
    "                        tag=\"total_loss\", scalar_value=(loss_contrastivep.item()+loss_contrastiveu.item()), global_step = step\n",
    "                    )\n",
    "                    # check if we log images in this iteration\n",
    "                    # if step % log_image_interval == 0:\n",
    "                    #     tb_logger.add_images(\n",
    "                    #         tag=\"in_unpaired1\", img_tensor=unpaired1.to(\"cpu\"), global_step=step\n",
    "                    #     )\n",
    "                    #     tb_logger.add_images(\n",
    "                    #         tag=\"in_unpaired2\", img_tensor=unpaired2.to(\"cpu\"), global_step=step\n",
    "                    #     )\n",
    "                    #     tb_logger.add_images(\n",
    "                    #         tag=\"in_paired1\", img_tensor=paired1.to(\"cpu\"), global_step=step\n",
    "                    #     )\n",
    "                    #     tb_logger.add_images(\n",
    "                    #         tag=\"in_paired2\", img_tensor=paired2.to(\"cpu\"), global_step=step\n",
    "                    #     )\n",
    "\n",
    "\n",
    "            print(f\"epoch {epoch}, total_loss = {epoch_loss}, positive_loss={epoch_loss_pos}, negative_loss={epoch_loss_neg}\")\n",
    "            \n",
    "            if(epoch % 2 == 0):\n",
    "                baseP = '/mnt/shared/celltracking/modelstates/'\n",
    "                machine = 'rerun_32output_augment_run2'\n",
    "                e = epoch \n",
    "                saveP = (baseP+'/'+machine+'/'+str(f'epoch_{e}'))\n",
    "                torch.save(model.state_dict(), saveP)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(tb_logger = logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
