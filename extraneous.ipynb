{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02519b47-cae0-4ad9-ac19-372beedf7c36",
   "metadata": {},
   "source": [
    "# Other VGG Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "467e4d38-c30e-40b5-bba7-d3ea8e874c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGG(nn.Module):\n",
    "\n",
    "#     def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "#         super(VGG, self).__init__()\n",
    "#         self.features = features\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(512 * 7 * 7, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         if init_weights:\n",
    "#             self._initialize_weights()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                 nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213099ce-b300-4be1-9003-1e74c4f72ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Vgg2D(torch.nn.Module):\n",
    "\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_size,\n",
    "#             fmaps=12,\n",
    "#             downsample_factors=[(2, 2), (2, 2), (2, 2), (2, 2)],\n",
    "#             output_classes=6):\n",
    "\n",
    "#         super(Vgg2D, self).__init__()\n",
    "\n",
    "#         self.input_size = input_size\n",
    "\n",
    "#         current_fmaps, h, w = tuple(input_size)\n",
    "#         current_size = (h, w)\n",
    "\n",
    "#         features = []\n",
    "#         for i in range(len(downsample_factors)):\n",
    "\n",
    "#             features += [\n",
    "#                 torch.nn.Conv2d(\n",
    "#                     current_fmaps,\n",
    "#                     fmaps,\n",
    "#                     kernel_size=3,\n",
    "#                     padding=1),\n",
    "#                 torch.nn.BatchNorm2d(fmaps),\n",
    "#                 torch.nn.ReLU(inplace=True),\n",
    "#                 torch.nn.Conv2d(\n",
    "#                     fmaps,\n",
    "#                     fmaps,\n",
    "#                     kernel_size=3,\n",
    "#                     padding=1),\n",
    "#                 torch.nn.BatchNorm2d(fmaps),\n",
    "#                 torch.nn.ReLU(inplace=True),\n",
    "#                 torch.nn.MaxPool2d(downsample_factors[i])\n",
    "#             ]\n",
    "\n",
    "#             current_fmaps = fmaps\n",
    "#             fmaps *= 2\n",
    "\n",
    "#             size = tuple(\n",
    "#                 int(c/d)\n",
    "#                 for c, d in zip(current_size, downsample_factors[i]))\n",
    "#             check = (\n",
    "#                 s*d == c\n",
    "#                 for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "#             assert all(check), \\\n",
    "#                 \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "#                 (current_size,)\n",
    "#             current_size = size\n",
    "\n",
    "#         self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "#         classifier = [\n",
    "#             torch.nn.Linear(\n",
    "#                 current_size[0] *\n",
    "#                 current_size[1] *\n",
    "#                 current_fmaps,\n",
    "#                 4096),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Dropout(),\n",
    "#             torch.nn.Linear(\n",
    "#                 4096,\n",
    "#                 4096),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Dropout(),\n",
    "#             torch.nn.Linear(\n",
    "#                 4096,\n",
    "#                 output_classes)\n",
    "#         ]\n",
    "\n",
    "#         self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "#     def forward(self, raw):\n",
    "\n",
    "#         # add a channel dimension to raw\n",
    "#         # shape = tuple(raw.shape)\n",
    "#         # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "#         # compute features\n",
    "#         f = self.features(raw)\n",
    "#         f = f.view(f.size(0), -1)\n",
    "        \n",
    "#         # classify\n",
    "#         y = self.classifier(f)\n",
    "\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ceea68-134d-4bb0-b1a1-fb125e8d7ade",
   "metadata": {},
   "source": [
    "# Other Training frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39fc7b-725e-4abc-ac9c-0b3301eac450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on fake data\n",
    "def train(model, loss_function):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        # x = np.stack(x, axis= 0)\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        x = x[np.newaxis]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        print(x.shape)\n",
    "        print(y.dtype)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # zero the gradients for this iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # apply model and calculate loss\n",
    "        prediction = model(x)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        # backpropagate the loss and adjust the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # log to console\n",
    "        # if batch_id % log_interval == 0:\n",
    "        #     print(\n",
    "        #         \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "        #             epoch,\n",
    "        #             batch_id * len(x),\n",
    "        #             len(loader.dataset),\n",
    "        #             100.0 * batch_id / len(loader),\n",
    "        #             loss.item(),\n",
    "        #         )\n",
    "        #     )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
