{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fa3a08-111f-4d26-8e65-7be9ba004054",
   "metadata": {},
   "source": [
    "# Model Implementation for 3D Cell Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503c9e5-eea9-4751-9600-fd8c537aecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae23f64-6f4c-42b5-b523-43668c65f2f7",
   "metadata": {},
   "source": [
    "## Designed VGG Model from Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2716b1-af41-47db-a7bc-4036b86a5823",
   "metadata": {},
   "source": [
    "We will use a VGG network to classify the synapse images. The input to the network will be a 2D image as provided by your dataloader. The output will be a vector of six floats, corresponding to the probability of the input to belong to the six classes.\n",
    "\n",
    "Implement a VGG network with the following specificatons:\n",
    "\n",
    "* the constructor takes the size of the 2D input image as height and width\n",
    "* the network starts with a downsample path consisting of:\n",
    "    * one convolutional layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.Conv2d` layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.MaxPool2d` with a `downsample_factor` of (2, 2) at each level\n",
    "* followed by three more downsampling paths like the one above, every time doubling the number of `fmaps` (i.e., the second one will have 24, the third 48, and the fourth 96). Make sure to keep track of the `current_fmaps` each time!\n",
    "* then two times:\n",
    "    * `nn.Linear` layer with `out_features=4096`. Be careful withe in `in_features` of the first one, which will depend on the size of the previous output!\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.DropOut`\n",
    "* Finally, one more fully connected layer with\n",
    "    * `nn.Linear` to the 6 classes\n",
    "    * no activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5e4a8-6b2c-4de7-9817-7510aafe3a70",
   "metadata": {},
   "source": [
    "Original One (from https://blog.paperspace.com/vgg-from-scratch-pytorch/)\n",
    "https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205ad50-7c0e-4181-b291-f5b505a85e94",
   "metadata": {},
   "source": [
    "# Create and Load Artificial Data\n",
    "Make a fake dataset to test on the VGG model while waiting for data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d66eb46f-2c07-4ec9-a277-f9d570519b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data size = 512x712x34 \n",
    "\n",
    "fd_class1 = np.random.randint(low=30,high=60,size=(128,128, 128),dtype='int')\n",
    "fd_class2 = np.random.randint(low=0,high=29,size=(128,128, 128),dtype='int')\n",
    "y1 = 1\n",
    "y2 = 0\n",
    "fd_class1 = np.expand_dims(fd_class1, axis=0)\n",
    "fd_class2 = np.expand_dims(fd_class2, axis=0)\n",
    "loader = [(fd_class1, y1), (fd_class2,y2)]\n",
    "#train = np.ndarray.flatten(fd_class1)\n",
    "#train2 = np.ndarray.flatten(fd_class2)\n",
    "\n",
    "# train_dataset, validation_dataset, test_dataset = random_split(\n",
    "#     full_dataset,\n",
    "#     [num_training, num_validation, num_test],\n",
    "#     generator=torch.Generator().manual_seed(23061912))\n",
    "\n",
    "#dataloader = DataLoader(train_dataset, batch_size=8, drop_last=True, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f74b5-d746-4fe4-8661-2f5efcc0e95b",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4517e490-95cb-4786-b8d8-52b7a051da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_classes, downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.downsample_factors = downsample_factors\n",
    "        self.output_classes = 2\n",
    "\n",
    "        current_fmaps, h, w, d = tuple(input_size)\n",
    "        current_size = (h, w,d)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1]*current_size[2] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2402c-a5d9-4538-88e8-4dac17d5bda7",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "15fd6961-5904-42c4-9fd7-190c0685c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/bourquea/miniconda3/envs/09_knowledge_extraction/lib/python3.10/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c99f9b9c-984b-4130-b461-687a1b753ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 12, 128, 128, 128]             336\n",
      "       BatchNorm3d-2    [-1, 12, 128, 128, 128]              24\n",
      "              ReLU-3    [-1, 12, 128, 128, 128]               0\n",
      "            Conv3d-4    [-1, 12, 128, 128, 128]           3,900\n",
      "       BatchNorm3d-5    [-1, 12, 128, 128, 128]              24\n",
      "              ReLU-6    [-1, 12, 128, 128, 128]               0\n",
      "         MaxPool3d-7       [-1, 12, 64, 64, 64]               0\n",
      "            Conv3d-8       [-1, 24, 64, 64, 64]           7,800\n",
      "       BatchNorm3d-9       [-1, 24, 64, 64, 64]              48\n",
      "             ReLU-10       [-1, 24, 64, 64, 64]               0\n",
      "           Conv3d-11       [-1, 24, 64, 64, 64]          15,576\n",
      "      BatchNorm3d-12       [-1, 24, 64, 64, 64]              48\n",
      "             ReLU-13       [-1, 24, 64, 64, 64]               0\n",
      "        MaxPool3d-14       [-1, 24, 32, 32, 32]               0\n",
      "           Conv3d-15       [-1, 48, 32, 32, 32]          31,152\n",
      "      BatchNorm3d-16       [-1, 48, 32, 32, 32]              96\n",
      "             ReLU-17       [-1, 48, 32, 32, 32]               0\n",
      "           Conv3d-18       [-1, 48, 32, 32, 32]          62,256\n",
      "      BatchNorm3d-19       [-1, 48, 32, 32, 32]              96\n",
      "             ReLU-20       [-1, 48, 32, 32, 32]               0\n",
      "        MaxPool3d-21       [-1, 48, 16, 16, 16]               0\n",
      "           Conv3d-22       [-1, 96, 16, 16, 16]         124,512\n",
      "      BatchNorm3d-23       [-1, 96, 16, 16, 16]             192\n",
      "             ReLU-24       [-1, 96, 16, 16, 16]               0\n",
      "           Conv3d-25       [-1, 96, 16, 16, 16]         248,928\n",
      "      BatchNorm3d-26       [-1, 96, 16, 16, 16]             192\n",
      "             ReLU-27       [-1, 96, 16, 16, 16]               0\n",
      "        MaxPool3d-28          [-1, 96, 8, 8, 8]               0\n",
      "           Linear-29                 [-1, 4096]     201,330,688\n",
      "             ReLU-30                 [-1, 4096]               0\n",
      "          Dropout-31                 [-1, 4096]               0\n",
      "           Linear-32                 [-1, 4096]      16,781,312\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "           Linear-35                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 218,672,732\n",
      "Trainable params: 218,672,732\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 1562.06\n",
      "Params size (MB): 834.17\n",
      "Estimated Total Size (MB): 2404.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff22bf-d49f-42bf-b92d-b110037d97fc",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "We'll probably need to test some different loss functions. List some here:\n",
    "Contrastive loss\n",
    "cosine similarity\n",
    "triplet loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c52a2b5-1749-4209-b022-41a445109799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"Contrastive loss function\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "            + (label)\n",
    "            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "61df0d0a-905c-4d24-8444-6ba095d24f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1, 128, 128, 128)\n",
    "downsample_factors =[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)];\n",
    "output_classes = 16\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg3D(input_size, output_classes,  downsample_factors = downsample_factors)\n",
    "model = model.to(device)\n",
    "\n",
    "#Training length\n",
    "epochs = 20\n",
    "\n",
    "#loss_function = torch.nn.BCELoss()\n",
    "#loss_function = torch.nn.CosineSimilarity()\n",
    "loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8ae0a-3ca1-476b-8055-32e418c702fd",
   "metadata": {},
   "source": [
    "# Implementing the Siamese Network\n",
    "\n",
    "The above training is just to test if the VGG model works for 3D data. Here, the training will take two pairs of images and calculate the loss from both pairs of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3afd5827-5f98-435c-b1c6-9c212d20dbf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m net\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#set the device to cuda\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m iteration_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m):\n\u001b[1;32m     13\u001b[0m         vol0, vol1 , label \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     14\u001b[0m         vol0, vol1 , label \u001b[38;5;241m=\u001b[39m img0\u001b[38;5;241m.\u001b[39mto(device), img1\u001b[38;5;241m.\u001b[39mto(device) , label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    loss=[] \n",
    "    counter=[]\n",
    "    iteration_number = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            vol0, vol1 , label = data\n",
    "            vol0, vol1 , label = img0.to(device), img1.to(device) , label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output1 = model(vol0)\n",
    "            output2 = model(vol1)\n",
    "            \n",
    "            loss_contrastive = loss_function(output1,output2,label)\n",
    "            \n",
    "            loss_contrastive.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "        print(\"Epoch {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n",
    "        iteration_number += 10\n",
    "        counter.append(iteration_number)\n",
    "        loss.append(loss_contrastive.item())\n",
    "    \n",
    "    show_plot(counter, loss)   \n",
    "    return net\n",
    "\n",
    "\n",
    "#set the device to cuda\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703be9f0-3f8c-4102-8288-13b2f55d5fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
