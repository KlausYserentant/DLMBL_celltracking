{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fa3a08-111f-4d26-8e65-7be9ba004054",
   "metadata": {},
   "source": [
    "# Model Implementation for 3D Cell Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503c9e5-eea9-4751-9600-fd8c537aecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae23f64-6f4c-42b5-b523-43668c65f2f7",
   "metadata": {},
   "source": [
    "## Designed VGG Model from Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2716b1-af41-47db-a7bc-4036b86a5823",
   "metadata": {},
   "source": [
    "We will use a VGG network to classify the synapse images. The input to the network will be a 2D image as provided by your dataloader. The output will be a vector of six floats, corresponding to the probability of the input to belong to the six classes.\n",
    "\n",
    "Implement a VGG network with the following specificatons:\n",
    "\n",
    "* the constructor takes the size of the 2D input image as height and width\n",
    "* the network starts with a downsample path consisting of:\n",
    "    * one convolutional layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.Conv2d` layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.MaxPool2d` with a `downsample_factor` of (2, 2) at each level\n",
    "* followed by three more downsampling paths like the one above, every time doubling the number of `fmaps` (i.e., the second one will have 24, the third 48, and the fourth 96). Make sure to keep track of the `current_fmaps` each time!\n",
    "* then two times:\n",
    "    * `nn.Linear` layer with `out_features=4096`. Be careful withe in `in_features` of the first one, which will depend on the size of the previous output!\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.DropOut`\n",
    "* Finally, one more fully connected layer with\n",
    "    * `nn.Linear` to the 6 classes\n",
    "    * no activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5e4a8-6b2c-4de7-9817-7510aafe3a70",
   "metadata": {},
   "source": [
    "Original One (from https://blog.paperspace.com/vgg-from-scratch-pytorch/)\n",
    "https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205ad50-7c0e-4181-b291-f5b505a85e94",
   "metadata": {},
   "source": [
    "# Create and Load Artificial Data\n",
    "Make a fake dataset to test on the VGG model while waiting for data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d66eb46f-2c07-4ec9-a277-f9d570519b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data size = 512x712x34 \n",
    "\n",
    "fd_class1 = np.random.randint(low=30,high=60,size=(128,128, 128),dtype='int')\n",
    "fd_class2 = np.random.randint(low=0,high=29,size=(128,128, 128),dtype='int')\n",
    "y1 = 1\n",
    "y2 = 0\n",
    "fd_class1 = np.expand_dims(fd_class1, axis=0)\n",
    "fd_class2 = np.expand_dims(fd_class2, axis=0)\n",
    "loader = [(fd_class1, y1), (fd_class2,y2)]\n",
    "#train = np.ndarray.flatten(fd_class1)\n",
    "#train2 = np.ndarray.flatten(fd_class2)\n",
    "\n",
    "# train_dataset, validation_dataset, test_dataset = random_split(\n",
    "#     full_dataset,\n",
    "#     [num_training, num_validation, num_test],\n",
    "#     generator=torch.Generator().manual_seed(23061912))\n",
    "\n",
    "#dataloader = DataLoader(train_dataset, batch_size=8, drop_last=True, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "285f74d9-d90e-40dc-aef7-4b91d9a314c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee27185b-cf37-4411-a9e8-ff2c21f4b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 128, 128, 128)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fd_class1.shape)\n",
    "test = np.expand_dims(fd_class1, axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f74b5-d746-4fe4-8661-2f5efcc0e95b",
   "metadata": {},
   "source": [
    "# My Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4517e490-95cb-4786-b8d8-52b7a051da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_classes, downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.downsample_factors = downsample_factors\n",
    "        self.output_classes = 2\n",
    "\n",
    "        current_fmaps, h, w, d = tuple(input_size)\n",
    "        current_size = (h, w,d)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1]*current_size[2] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "78620224-08d9-4faa-9b7b-55ef3b49a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of our images\n",
    "# for x, y in train_dataset:\n",
    "#     input_size = x.shape\n",
    "#     break\n",
    "input_size = (1, 128, 128, 128)\n",
    "downsample_factors =[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)];\n",
    "output_classes = 2\n",
    "# create the model to train\n",
    "model = Vgg3D(input_size, output_classes,  downsample_factors = downsample_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fad08644-627d-4480-bf2d-89ec65361791",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "#loss = torch.nn.CosineSimilarity()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d6325fe-0e76-46bb-81b3-6cef96fc890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use device cuda for training\n"
     ]
    }
   ],
   "source": [
    "# use a GPU, if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Will use device {device} for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2402c-a5d9-4538-88e8-4dac17d5bda7",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15fd6961-5904-42c4-9fd7-190c0685c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c99f9b9c-984b-4130-b461-687a1b753ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 12, 128, 128, 128]             336\n",
      "       BatchNorm3d-2    [-1, 12, 128, 128, 128]              24\n",
      "              ReLU-3    [-1, 12, 128, 128, 128]               0\n",
      "            Conv3d-4    [-1, 12, 128, 128, 128]           3,900\n",
      "       BatchNorm3d-5    [-1, 12, 128, 128, 128]              24\n",
      "              ReLU-6    [-1, 12, 128, 128, 128]               0\n",
      "         MaxPool3d-7       [-1, 12, 64, 64, 64]               0\n",
      "            Conv3d-8       [-1, 24, 64, 64, 64]           7,800\n",
      "       BatchNorm3d-9       [-1, 24, 64, 64, 64]              48\n",
      "             ReLU-10       [-1, 24, 64, 64, 64]               0\n",
      "           Conv3d-11       [-1, 24, 64, 64, 64]          15,576\n",
      "      BatchNorm3d-12       [-1, 24, 64, 64, 64]              48\n",
      "             ReLU-13       [-1, 24, 64, 64, 64]               0\n",
      "        MaxPool3d-14       [-1, 24, 32, 32, 32]               0\n",
      "           Conv3d-15       [-1, 48, 32, 32, 32]          31,152\n",
      "      BatchNorm3d-16       [-1, 48, 32, 32, 32]              96\n",
      "             ReLU-17       [-1, 48, 32, 32, 32]               0\n",
      "           Conv3d-18       [-1, 48, 32, 32, 32]          62,256\n",
      "      BatchNorm3d-19       [-1, 48, 32, 32, 32]              96\n",
      "             ReLU-20       [-1, 48, 32, 32, 32]               0\n",
      "        MaxPool3d-21       [-1, 48, 16, 16, 16]               0\n",
      "           Conv3d-22       [-1, 96, 16, 16, 16]         124,512\n",
      "      BatchNorm3d-23       [-1, 96, 16, 16, 16]             192\n",
      "             ReLU-24       [-1, 96, 16, 16, 16]               0\n",
      "           Conv3d-25       [-1, 96, 16, 16, 16]         248,928\n",
      "      BatchNorm3d-26       [-1, 96, 16, 16, 16]             192\n",
      "             ReLU-27       [-1, 96, 16, 16, 16]               0\n",
      "        MaxPool3d-28          [-1, 96, 8, 8, 8]               0\n",
      "           Linear-29                 [-1, 4096]     201,330,688\n",
      "             ReLU-30                 [-1, 4096]               0\n",
      "          Dropout-31                 [-1, 4096]               0\n",
      "           Linear-32                 [-1, 4096]      16,781,312\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "          Dropout-34                 [-1, 4096]               0\n",
      "           Linear-35                    [-1, 2]           8,194\n",
      "================================================================\n",
      "Total params: 218,615,374\n",
      "Trainable params: 218,615,374\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 1562.06\n",
      "Params size (MB): 833.95\n",
      "Estimated Total Size (MB): 2404.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "summary( model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "57eba547-a544-4c6a-9486-f47578156ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 128)\n",
      "(1, 128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eb80e0d9-29b5-4f2e-a8c6-ade81ed98501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on fake data\n",
    "def train(model, loss_function):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        # x = np.stack(x, axis= 0)\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        x = x[np.newaxis]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        print(x.shape)\n",
    "        print(y.dtype)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # zero the gradients for this iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # apply model and calculate loss\n",
    "        prediction = model(x)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        # backpropagate the loss and adjust the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # log to console\n",
    "        # if batch_id % log_interval == 0:\n",
    "        #     print(\n",
    "        #         \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "        #             epoch,\n",
    "        #             batch_id * len(x),\n",
    "        #             len(loader.dataset),\n",
    "        #             100.0 * batch_id / len(loader),\n",
    "        #             loss.item(),\n",
    "        #         )\n",
    "        #     )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "656931c2-c6fa-42b2-9f9c-7a7f2fa0ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.float32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, training loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_function)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# apply model and calculate loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# backpropagate the loss and adjust the parameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/09_knowledge_extraction/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/09_knowledge_extraction/lib/python3.10/site-packages/torch/nn/modules/loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/09_knowledge_extraction/lib/python3.10/site-packages/torch/nn/functional.py:3074\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3076\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3077\u001b[0m     )\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    \n",
    "    epoch_loss = train(model, loss)\n",
    "    print(f\"epoch {epoch}, training loss={epoch_loss}\")\n",
    "    \n",
    "    # accuracy = validate()\n",
    "    # print(f\"epoch {epoch}, validation accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df385c6f-d59e-45b1-b83a-a7e099f23ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[[34 48 39 ... 35 35 51]\n",
      "  [58 58 31 ... 31 58 46]\n",
      "  [55 43 59 ... 34 50 36]\n",
      "  ...\n",
      "  [47 32 47 ... 33 30 50]\n",
      "  [44 57 31 ... 53 44 31]\n",
      "  [33 42 48 ... 33 44 39]]\n",
      "\n",
      " [[55 31 50 ... 42 36 41]\n",
      "  [35 58 56 ... 33 46 49]\n",
      "  [56 33 44 ... 49 36 51]\n",
      "  ...\n",
      "  [35 36 51 ... 56 53 33]\n",
      "  [37 40 50 ... 59 42 31]\n",
      "  [44 40 45 ... 37 45 34]]\n",
      "\n",
      " [[41 45 54 ... 34 37 46]\n",
      "  [51 40 58 ... 44 54 35]\n",
      "  [36 50 46 ... 39 40 45]\n",
      "  ...\n",
      "  [46 36 48 ... 40 45 58]\n",
      "  [31 40 49 ... 37 44 48]\n",
      "  [43 40 54 ... 58 44 57]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[37 59 52 ... 49 36 30]\n",
      "  [58 49 49 ... 48 56 56]\n",
      "  [54 40 58 ... 53 38 37]\n",
      "  ...\n",
      "  [37 33 31 ... 46 44 44]\n",
      "  [39 45 50 ... 33 55 43]\n",
      "  [41 47 39 ... 57 59 32]]\n",
      "\n",
      " [[35 33 50 ... 41 34 50]\n",
      "  [30 43 46 ... 55 44 41]\n",
      "  [58 37 57 ... 34 43 52]\n",
      "  ...\n",
      "  [59 40 37 ... 41 36 35]\n",
      "  [31 33 38 ... 51 47 47]\n",
      "  [44 58 42 ... 54 44 42]]\n",
      "\n",
      " [[50 33 42 ... 39 42 33]\n",
      "  [56 40 56 ... 48 48 41]\n",
      "  [36 46 57 ... 35 58 42]\n",
      "  ...\n",
      "  [36 34 43 ... 46 33 57]\n",
      "  [34 45 57 ... 31 52 45]\n",
      "  [33 42 36 ... 51 53 54]]]\n",
      "y:  1\n",
      "bat: 0\n",
      "x: [[[ 6  4 22 ... 20 11  6]\n",
      "  [ 0 25 20 ... 19  2  4]\n",
      "  [27 23 14 ... 14 10 11]\n",
      "  ...\n",
      "  [ 3 14 17 ... 21  6  5]\n",
      "  [19 10 15 ...  2 21 21]\n",
      "  [19  0  2 ... 28 24 24]]\n",
      "\n",
      " [[18 13 10 ...  2 21 26]\n",
      "  [ 8  7 10 ...  0  5  7]\n",
      "  [ 3 13 25 ... 18  3 14]\n",
      "  ...\n",
      "  [ 1  6 28 ... 16 25  2]\n",
      "  [15  2  0 ...  7 10 28]\n",
      "  [13  5 11 ...  3 16  0]]\n",
      "\n",
      " [[10 21  6 ... 26  3 10]\n",
      "  [25 27  0 ... 19 10 13]\n",
      "  [ 9 26 23 ...  0 21 18]\n",
      "  ...\n",
      "  [20 20 13 ...  8 27  1]\n",
      "  [ 0 27 17 ... 24 25 13]\n",
      "  [26 19  0 ...  9 15 20]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1 12 23 ... 27 15  6]\n",
      "  [ 2 23 24 ... 19 15 25]\n",
      "  [10 14 27 ... 16  3  7]\n",
      "  ...\n",
      "  [28 20 18 ... 25 26 15]\n",
      "  [21 10  3 ...  6 21  8]\n",
      "  [12  3  4 ... 10  1 15]]\n",
      "\n",
      " [[ 4  1  8 ... 11 21 13]\n",
      "  [20 13 17 ... 15 25  3]\n",
      "  [23  5  0 ...  8 14 11]\n",
      "  ...\n",
      "  [ 0 12 24 ... 27  2 17]\n",
      "  [26 27 17 ...  5 19  7]\n",
      "  [15 28 28 ... 27  3  8]]\n",
      "\n",
      " [[21 20  6 ... 23 15  4]\n",
      "  [25 14  1 ...  8 20  4]\n",
      "  [ 5 15 12 ... 28 18  7]\n",
      "  ...\n",
      "  [27 25  2 ... 17 15 20]\n",
      "  [ 3 28  8 ... 12  8 15]\n",
      "  [12 23 20 ...  8  7  1]]]\n",
      "y:  0\n",
      "bat: 1\n"
     ]
    }
   ],
   "source": [
    "for batch_id, (x,y) in enumerate(loader):\n",
    "    print('x:', x)\n",
    "    print('y: ', y)\n",
    "    print('bat:', batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988160f6-b60a-4887-8bd6-10a6803129f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def train():\n",
    "#     '''Train the model for one epoch.'''\n",
    "\n",
    "#     # set the model into train mode\n",
    "#     model.train()\n",
    "\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     num_batches = 0\n",
    "#     for x, y in tqdm(dataloader, 'train'):\n",
    "\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         y_pred = model(x)\n",
    "#         l = loss(y_pred, y)\n",
    "#         l.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += l\n",
    "#         num_batches += 1\n",
    "\n",
    "#     return epoch_loss/num_batches\n",
    "\n",
    "# def evaluate(dataloader, name):\n",
    "    \n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for x, y in tqdm(dataloader, name):\n",
    "        \n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         logits = model(x)\n",
    "#         probs = torch.nn.Softmax(dim=1)(logits)\n",
    "#         predictions = torch.argmax(probs, dim=1)\n",
    "        \n",
    "#         correct += int(torch.sum(predictions == y).cpu().detach().numpy())\n",
    "#         total += len(y)\n",
    "    \n",
    "#     accuracy = correct/total\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "# def validate():\n",
    "#     '''Evaluate prediction accuracy on the validation dataset.'''\n",
    "    \n",
    "#     model.eval()\n",
    "#     dataloader = DataLoader(validation_dataset, batch_size=32)\n",
    "   \n",
    "#     return evaluate(dataloader, 'validate')\n",
    "\n",
    "# def test():\n",
    "#     '''Evaluate prediction accuracy on the test dataset.'''\n",
    "    \n",
    "#     model.eval()\n",
    "#     dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "#     return evaluate(dataloader, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cca941-4b87-4ee3-a20c-b69612b58d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(3):\n",
    "    \n",
    "#     epoch_loss = train()\n",
    "#     print(f\"epoch {epoch}, training loss={epoch_loss}\")\n",
    "    \n",
    "#     accuracy = validate()\n",
    "#     print(f\"epoch {epoch}, validation accuracy={accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1847c3a-6b60-48bd-aef6-6fee20c7438d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
