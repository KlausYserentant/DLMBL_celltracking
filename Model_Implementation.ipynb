{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fa3a08-111f-4d26-8e65-7be9ba004054",
   "metadata": {},
   "source": [
    "# Model Implementation for 3D Cell Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2503c9e5-eea9-4751-9600-fd8c537aecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae23f64-6f4c-42b5-b523-43668c65f2f7",
   "metadata": {},
   "source": [
    "## Designed VGG Model from Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2716b1-af41-47db-a7bc-4036b86a5823",
   "metadata": {},
   "source": [
    "We will use a VGG network to classify the synapse images. The input to the network will be a 2D image as provided by your dataloader. The output will be a vector of six floats, corresponding to the probability of the input to belong to the six classes.\n",
    "\n",
    "Implement a VGG network with the following specificatons:\n",
    "\n",
    "* the constructor takes the size of the 2D input image as height and width\n",
    "* the network starts with a downsample path consisting of:\n",
    "    * one convolutional layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.Conv2d` layer, kernel size (3, 3), to create 12 `fmaps`\n",
    "    * `nn.BatchNorm2d` over those feature maps\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.MaxPool2d` with a `downsample_factor` of (2, 2) at each level\n",
    "* followed by three more downsampling paths like the one above, every time doubling the number of `fmaps` (i.e., the second one will have 24, the third 48, and the fourth 96). Make sure to keep track of the `current_fmaps` each time!\n",
    "* then two times:\n",
    "    * `nn.Linear` layer with `out_features=4096`. Be careful withe in `in_features` of the first one, which will depend on the size of the previous output!\n",
    "    * `nn.ReLU` activation function\n",
    "    * `nn.DropOut`\n",
    "* Finally, one more fully connected layer with\n",
    "    * `nn.Linear` to the 6 classes\n",
    "    * no activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fda3134-5b72-40cd-97df-9eeda8a333c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            fmaps=12,\n",
    "            downsample_factors=[(2, 2), (2, 2), (2, 2), (2, 2)],\n",
    "            output_classes=6):\n",
    "\n",
    "        super(Vgg2D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        current_fmaps, h, w = tuple(input_size)\n",
    "        current_size = (h, w)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv2d(\n",
    "                    current_fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size=3,\n",
    "                    padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(\n",
    "                    fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size=3,\n",
    "                    padding=1),\n",
    "                torch.nn.BatchNorm2d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool2d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(\n",
    "                current_size[0] *\n",
    "                current_size[1] *\n",
    "                current_fmaps,\n",
    "                4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(\n",
    "                4096,\n",
    "                4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(\n",
    "                4096,\n",
    "                output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5e4a8-6b2c-4de7-9817-7510aafe3a70",
   "metadata": {},
   "source": [
    "Another One (from https://blog.paperspace.com/vgg-from-scratch-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1bdb340-eba9-4f88-b913-f96268fcc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205ad50-7c0e-4181-b291-f5b505a85e94",
   "metadata": {},
   "source": [
    "# Artificial Data\n",
    "Make a fake dataset to test on the VGG model while waiting for data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66eb46f-2c07-4ec9-a277-f9d570519b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data size = 512x712x34 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f74b5-d746-4fe4-8661-2f5efcc0e95b",
   "metadata": {},
   "source": [
    "# My Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4517e490-95cb-4786-b8d8-52b7a051da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size,downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        current_fmaps, h, w = tuple(input_size)\n",
    "        current_size = (h, w)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78620224-08d9-4faa-9b7b-55ef3b49a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of our images\n",
    "# for x, y in train_dataset:\n",
    "#     input_size = x.shape\n",
    "#     break\n",
    "input_size = (128, 128, 128)\n",
    "downsample_factors =[(2, 2,2), (2, 2,2), (2, 2,2), (2, 2,2)];\n",
    "# create the model to train\n",
    "model = Vgg2D(input_size, downsample_factors = downsample_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad08644-627d-4480-bf2d-89ec65361791",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCosineSimilarity()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CosineSimilarity()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2402c-a5d9-4538-88e8-4dac17d5bda7",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988160f6-b60a-4887-8bd6-10a6803129f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    '''Train the model for one epoch.'''\n",
    "\n",
    "    # set the model into train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    for x, y in tqdm(dataloader, 'train'):\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += l\n",
    "        num_batches += 1\n",
    "\n",
    "    return epoch_loss/num_batches\n",
    "\n",
    "def evaluate(dataloader, name):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in tqdm(dataloader, name):\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)\n",
    "        predictions = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        correct += int(torch.sum(predictions == y).cpu().detach().numpy())\n",
    "        total += len(y)\n",
    "    \n",
    "    accuracy = correct/total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def validate():\n",
    "    '''Evaluate prediction accuracy on the validation dataset.'''\n",
    "    \n",
    "    model.eval()\n",
    "    dataloader = DataLoader(validation_dataset, batch_size=32)\n",
    "   \n",
    "    return evaluate(dataloader, 'validate')\n",
    "\n",
    "def test():\n",
    "    '''Evaluate prediction accuracy on the test dataset.'''\n",
    "    \n",
    "    model.eval()\n",
    "    dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    return evaluate(dataloader, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
