{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf7b740-20f5-4f71-9b15-aa3a5cbdfb8b",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5244ea29-c3fc-421b-bc23-b40b99f85456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gunpowder as gp\n",
    "import zarr\n",
    "import math\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import skimage\n",
    "import networkx\n",
    "import pathlib\n",
    "from tifffile import imread, imwrite\n",
    "import tensorboard\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a113d5-b59e-4c88-b972-97cde7b96e74",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd2a1dc6-0b71-4b60-839c-c5ec5fb460dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_classes, downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.downsample_factors = downsample_factors\n",
    "        self.output_classes = 2\n",
    "\n",
    "        current_fmaps, h, w, d = tuple(input_size)\n",
    "        current_size = (h, w,d)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1]*current_size[2] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a388de4-2da7-4936-9b94-440e27bf99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1, 64, 64, 5)\n",
    "downsample_factors =[(2, 2, 1), (2, 2, 1), (2, 2, 1), (2, 2, 1)];\n",
    "output_classes = 12\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg3D(input_size, output_classes,  downsample_factors = downsample_factors)\n",
    "model = model.to(device)\n",
    "\n",
    "#summary(model, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976eba0b-7343-43cb-b03f-24da5a599dbb",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0626de4a-0794-43a4-b085-2cf106f69791",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CosineEmbeddingLoss()\n",
    "#loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc893743-b752-4a8d-a390-69e095669baf",
   "metadata": {},
   "source": [
    "# convert data to zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dab1b07-9d35-4e5e-bac6-d0c1754625f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Convert data into zarr format\n",
    "# def tif2zarr(baseDir,groupKey,outDir='',chunkSize=48):\n",
    "#     if outDir=='':\n",
    "#         outDir = baseDir+\"/zarr/\"\n",
    "    \n",
    "#     # load all tiff files \n",
    "#     img = np.stack([imread(xi) for xi in sorted(glob.glob(baseDir+\"/\"+\"*.tif\"))])  # images\n",
    "\n",
    "#     # format: [timepoints,zslices,y,z]\n",
    "#     imDims = img.shape\n",
    "    \n",
    "#     # define chunk size\n",
    "#     if isinstance(chunkSize, int):\n",
    "#         chunks = np.repeat(chunkSize,len(imDims))\n",
    "#     else:\n",
    "#         chunks = chunkSize\n",
    "                  \n",
    "#     # write img into zarr file       \n",
    "#     #zFile = zarr.open(outDir, shape=imDims, chunks=chunks, mode=\"w\", dtype='int32')\n",
    "#     store = zarr.DirectoryStore(outDir)\n",
    "#     zFile = zarr.open_group(store=store,mode='w')\n",
    "        \n",
    "#     zGroup = zFile.create_dataset(groupKey, shape=imDims,chunks=chunks,dtype='int32')\n",
    "#     #zFile = zarr.open_group(outDir+\"/\"+groupKey,mode='w',shape=imDims, chunks=chunks, dtype='int32')\n",
    "#     zFile[groupKey] = img\n",
    "        \n",
    "#     return outDir\n",
    "\n",
    "# dataDir = \"/mnt/shared/celltracking/data/cho/02\"\n",
    "# zarr_dir = \"/mnt/shared/celltracking/data/cho/02.zarr\"\n",
    "# zarrdir = tif2zarr(dataDir,'raw',zarr_dir)\n",
    "# print(f'zarr data directory:{zarrdir}')\n",
    "zarrdir = '/mnt/shared/celltracking/data/cho/02.zarr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710453dc-555c-45e1-b6e2-7c08657c34db",
   "metadata": {},
   "source": [
    "# Read tracks from GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f197ab97-2fec-4f67-9168-71d8219f0dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# data summary\n",
      "total number of tracks: 13\n",
      "total number of edges: 596\n",
      "total number of nodes: 614\n",
      "-------\n",
      "track 0 has 154 edges and 155 nodes.\n",
      "track 1 has 139 edges and 140 nodes.\n",
      "track 2 has 103 edges and 104 nodes.\n",
      "track 3 has 25 edges and 26 nodes.\n",
      "track 4 has 2 edges and 3 nodes.\n",
      "track 5 has 2 edges and 3 nodes.\n",
      "track 6 has 9 edges and 10 nodes.\n",
      "track 7 has 55 edges and 56 nodes.\n",
      "track 8 has 4 edges and 5 nodes.\n",
      "track 9 has 1 edges and 2 nodes.\n",
      "track 10 has 42 edges and 43 nodes.\n",
      "track 11 has 39 edges and 40 nodes.\n",
      "track 12 has 21 edges and 22 nodes.\n"
     ]
    }
   ],
   "source": [
    "base_path = pathlib.Path(\"/mnt/shared/celltracking/data/cho/\")\n",
    "\n",
    "# read parent-child links from file\n",
    "links = np.loadtxt(base_path / \"02_GT/TRA\" / \"man_track.txt\", dtype=int)\n",
    "\n",
    "# read annotated image stack\n",
    "annotations = np.stack([imread(xi) for xi in sorted((base_path / \"02_GT/TRA\").glob(\"*.tif\"))])  # images\n",
    "\n",
    "# extract centroids from annotated image stacks\n",
    "tracks_raw = []\n",
    "for t, frame in enumerate(annotations):\n",
    "    centers = skimage.measure.regionprops(frame)\n",
    "    for c in centers:\n",
    "        tracks_raw.append([c.label, t, int(c.centroid[1]), int(c.centroid[2])])\n",
    "        \n",
    "# constructs graph \n",
    "tracks = np.array(tracks_raw)\n",
    "graph = networkx.DiGraph()\n",
    "for cell_id, t, x, y in tracks:\n",
    "    graph.add_node((cell_id,t), x=x, y=y, t=t)\n",
    "    \n",
    "for cell_id, t in graph.nodes():\n",
    "    if (cell_id, t+1) in graph.nodes():\n",
    "        graph.add_edge((cell_id, t), (cell_id,t+1))\n",
    "        \n",
    "\n",
    "for child_id, child_from, _, child_parent_id in links:\n",
    "    for parent_id, _, parent_to, _ in links:\n",
    "        if child_parent_id == parent_id:\n",
    "            graph.add_edge((parent_id, parent_to), (child_id, child_from))\n",
    "            \n",
    "# extract trajectories from graph set\n",
    "tracks = [graph.subgraph(c) for c in networkx.weakly_connected_components(graph) if len(c)>0]\n",
    "\n",
    "# remove tracks with 0 edges\n",
    "tracks = [track for track in tracks if len(track.edges)>0]\n",
    "\n",
    "\n",
    "print('############# data summary')\n",
    "print(f'total number of tracks: {len(tracks)}')\n",
    "print(f'total number of edges: {len(graph.edges)}')\n",
    "print(f'total number of nodes: {len(graph.nodes)}')\n",
    "print('-------')\n",
    "for i,x in enumerate(tracks):\n",
    "    print(f'track {i} has {len(tracks[i].edges)} edges and {len(tracks[i].nodes)} nodes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe50b1-8d5b-48a3-9e62-5701bb88f7f0",
   "metadata": {},
   "source": [
    "# Define function to make image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a777800-3585-4bcc-b5aa-fc79fd94950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getPaired(gp.BatchFilter):\n",
    "\n",
    "    def __init__(self, raw, raw_shift, tracks, paired=True):\n",
    "        self.raw = raw\n",
    "        self.raw_shift = raw_shift\n",
    "        self.tracks = tracks\n",
    "        self.paired = paired\n",
    "    \n",
    "    # _ref channel array is stored in raw_ref, while second volume in pair will be stored raw_new\n",
    "    def prepare(self, request):\n",
    "        # obtain volume coordinates from tracks                \n",
    "        deps = gp.BatchRequest()\n",
    "        vol1,vol2 = self.sampler(request)\n",
    "                \n",
    "        deps[self.raw] = gp.ArraySpec(roi=gp.Roi(vol1,request[self.raw].roi.get_shape()))\n",
    "        deps[self.raw_shift] = gp.ArraySpec(roi=gp.Roi(vol2,request[self.raw_shift].roi.get_shape()))\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    # required to inform downstream nodes about new array \n",
    "    def process(self, batch, request):\n",
    "        # create a new batch to hold the new array\n",
    "        out_batch = gp.Batch()\n",
    "\n",
    "        # create new array and store it in the batch\n",
    "        out_batch[self.raw_shift] = batch[self.raw_shift]\n",
    "        out_batch[self.raw] = batch[self.raw]\n",
    "        \n",
    "        #print(f'raw: {batch[self.raw].spec.roi}')\n",
    "        #print(batch[self.raw_shift].spec.roi)\n",
    "        \n",
    "        # make sure that coordinates for batch[raw] and batch[raw_shift] are reset to (0,0,0,0,0)\n",
    "        out_batch[self.raw].spec.roi = request[self.raw].roi\n",
    "        out_batch[self.raw_shift].spec.roi = request[self.raw_shift].roi\n",
    "\n",
    "        # return the new batch\n",
    "        return out_batch\n",
    "    \n",
    "    # select pairs of subvolumes from data\n",
    "    def sampler(self,request):\n",
    "        tracks = self.tracks\n",
    "        paired = self.paired\n",
    "        # choose connected nodes\n",
    "        # if self.paired:\n",
    "        if paired:\n",
    "            t0 = tracks[np.random.randint(0,len(tracks),1).item()]\n",
    "            e0 = list(t0.edges)[np.random.randint(len(list(t0.edges)))]\n",
    "            node0 = t0.nodes[e0[0]]\n",
    "            node1 = t0.nodes[e0[1]]\n",
    "            \n",
    "        # choose random unconnected nodes\n",
    "        else:\n",
    "            # randomly choose two tracks and make sure they are not identical\n",
    "            t0,t1 = np.random.randint(0,len(tracks),2)\n",
    "            while t0==t1:\n",
    "                t0,t1 = np.random.randint(0,len(tracks),2)\n",
    "\n",
    "            #print(f'trackids: {t0,t1}')\n",
    "            t0 = tracks[t0]\n",
    "            t1 = tracks[t1]\n",
    "\n",
    "            # choose random edges from each track\n",
    "            #print(f'number edges per track{len(list(t0.nodes)),len(list(t1.nodes))}')\n",
    "\n",
    "            r0 = np.random.randint(0,len(list(t0.nodes))) \n",
    "            r1 = np.random.randint(0,len(list(t1.nodes)))\n",
    "\n",
    "            node0 = t0.nodes[list(t0.nodes)[r0]]\n",
    "            node1 = t1.nodes[list(t1.nodes)[r1]]\n",
    "            \n",
    "\n",
    "\n",
    "        node0_xyt = [node0[\"x\"], node0[\"y\"], node0[\"t\"]]\n",
    "        node1_xyt = [node1[\"x\"], node1[\"y\"], node1[\"t\"]]\n",
    "\n",
    "        #print(f'input coord: {node0_xyt,node1_xyt}')\n",
    "\n",
    "        roi_in = request[self.raw_shift].roi.get_shape()\n",
    "        #t,z,y,x\n",
    "        coords_vol0 = (node0_xyt[2],0,node0_xyt[0]-(roi_in[2]/2),node0_xyt[1]-(roi_in[3]/2))\n",
    "        coords_vol1 = (node1_xyt[2],0,node1_xyt[0]-(roi_in[2]/2),node1_xyt[1]-(roi_in[3]/2))\n",
    "        #print(f'output coords - vol0: {coords_vol0}, vol1:{coords_vol1}')\n",
    "\n",
    "        return coords_vol0, coords_vol1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c149d-41f7-49bc-a874-8e7096df88d5",
   "metadata": {},
   "source": [
    "# gunpowder pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f88a724b-a2a8-4b2d-a592-ee72865a3390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROI: None, voxel size: None, interpolatable: None, non-spatial: False, dtype: None, placeholder: False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify subvolume size and volume source\n",
    "volSize = (1,5,64, 64)\n",
    "coord = (0,0,0,0)\n",
    "batch_size = 8\n",
    "\n",
    "zarrdir = '/mnt/shared/celltracking/data/cho/02.zarr'\n",
    "raw = gp.ArrayKey('raw')\n",
    "raw_shift = gp.ArrayKey('raw_shift')\n",
    "\n",
    "# create \"pipeline\" consisting only of a data source and prepare \n",
    "pipeline_paired = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "    ) + gp.Pad(raw_shift, None) + gp.Pad(raw, None) + getPaired(raw,raw_shift,tracks,paired=True))\n",
    "\n",
    "pipeline_unpaired = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "    ) + gp.Pad(raw_shift, None) + gp.Pad(raw, None) + getPaired(raw,raw_shift,tracks,paired=False))\n",
    "\n",
    "\n",
    "\n",
    "# specify request\n",
    "request = gp.BatchRequest()\n",
    "request[raw] = gp.Roi(coord, volSize)\n",
    "request[raw_shift] = gp.Roi(coord, volSize)\n",
    "\n",
    "gp.ArraySpec()\n",
    "#build the pipeline...\n",
    "# with gp.build(pipeline_paired):\n",
    "\n",
    "#   # ...and request a batch\n",
    "#   batch = pipeline_paired.request_batch(request)\n",
    "  \n",
    "# # show the content of the batch\n",
    "# print(f\"batch returned: {batch}\")\n",
    "\n",
    "# # plot first slice of volume\n",
    "# fig, axs = plt.subplots(1,2)\n",
    "# print(batch[raw].data.shape)\n",
    "# axs[0].imshow(np.flipud(batch[raw].data[0,0,0,:,:]))\n",
    "# axs[1].imshow(np.flipud(batch[raw_shift].data[0,0,0,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54174c-1da2-4c2a-8944-6bc6f9903d8a",
   "metadata": {},
   "source": [
    "# Function to get predictions on random image pairs from saved model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d77aadb4-7748-49d9-bc25-4aa8d7e4e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNet(pairs):\n",
    "    reps = pairs\n",
    "    ids = []\n",
    "    preds = []\n",
    "    with gp.build(pipeline_paired), gp.build(pipeline_unpaired):\n",
    "        for x in range(0,reps):\n",
    "            if x%2==0:\n",
    "                #print(f'batch:{x}, paired')\n",
    "                batch = pipeline_paired.request_batch(request)  \n",
    "                y = torch.from_numpy(np.array([1])).to(device).float()\n",
    "                ids.append(1)\n",
    "            else:\n",
    "                #print(f'batch:{x}, unpaired')\n",
    "                batch = pipeline_unpaired.request_batch(request)  \n",
    "                y = torch.from_numpy(np.array([-1])).to(device).float()\n",
    "                ids.append(0)\n",
    "\n",
    "            vol0 = batch[raw].data\n",
    "            vol0 = np.reshape(vol0, (1,64, 64, 5))\n",
    "            vol0 = np.expand_dims(vol0, axis =0)\n",
    "\n",
    "            \n",
    "            vol1 = batch[raw_shift].data\n",
    "            vol1 = np.reshape(vol1, (1,64, 64, 5))\n",
    "            vol1 = np.expand_dims(vol1, axis =0)\n",
    "    \n",
    "            vol0 = torch.from_numpy(vol0).to(device).float()\n",
    "            vol1 = torch.from_numpy(vol1).to(device).float()\n",
    "\n",
    "            pred1 = model(vol1)\n",
    "            pred0 = model(vol0)\n",
    "            \n",
    "            tmp = torch.nn.functional.cosine_similarity(pred1,pred0).item()\n",
    "            #loss_contrastive = loss_function(pred0,pred1,y)\n",
    "            #tmp = np.float32(loss_contrastive.detach().cpu())\n",
    "            preds.append(tmp)\n",
    "            \n",
    "            if x%10==0:\n",
    "                print(f'batch: {x}')\n",
    "\n",
    "    #print(len(preds))\n",
    "    return ids,np.float32(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46aff7-e1af-47e5-8805-c827e3dc70d0",
   "metadata": {},
   "source": [
    "# Losses on unseen data for training with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a4bc495-55ef-4868-ae4b-50c8123c938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 10\n",
      "batch: 20\n",
      "batch: 30\n",
      "batch: 40\n",
      "batch: 50\n",
      "batch: 60\n",
      "batch: 70\n",
      "batch: 80\n",
      "batch: 90\n",
      "batch: 100\n",
      "batch: 110\n",
      "batch: 120\n",
      "batch: 130\n",
      "batch: 140\n",
      "batch: 150\n",
      "batch: 160\n",
      "batch: 170\n",
      "batch: 180\n",
      "batch: 190\n",
      "batch: 200\n",
      "batch: 210\n",
      "batch: 220\n",
      "batch: 230\n",
      "batch: 240\n",
      "batch: 250\n",
      "batch: 260\n",
      "batch: 270\n",
      "batch: 280\n",
      "batch: 290\n",
      "batch: 300\n",
      "batch: 310\n",
      "batch: 320\n",
      "batch: 330\n",
      "batch: 340\n",
      "batch: 350\n",
      "batch: 360\n",
      "batch: 370\n",
      "batch: 380\n",
      "batch: 390\n",
      "batch: 400\n",
      "batch: 410\n",
      "batch: 420\n",
      "batch: 430\n",
      "batch: 440\n",
      "batch: 450\n",
      "batch: 460\n",
      "batch: 470\n",
      "batch: 480\n",
      "batch: 490\n",
      "mean loss paired (no augmentation): 0.6130592823028564\n",
      "mean loss unpaired (no augmentation): 0.05225348100066185\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load model\n",
    "modelstateP = '/mnt/shared/celltracking/modelstates/klaus/'\n",
    "stateFile = 'epoch_300'\n",
    "model.load_state_dict(torch.load(modelstateP+stateFile))\n",
    "\n",
    "## set model to eval mode and compute predictions\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "ids_aug,preds_aug = evalNet(500)\n",
    "\n",
    "## reaarrange predictions\n",
    "loss_paired_aug = preds_aug[[i for i, x in enumerate(ids_aug) if x==1]]\n",
    "loss_unpaired_aug = preds_aug[[i for i, x in enumerate(ids_aug) if x==0]]\n",
    "print(f'mean loss paired (no augmentation): {np.mean(loss_paired_aug)}')\n",
    "print(f'mean loss unpaired (no augmentation): {np.mean(loss_unpaired_aug)}')\n",
    "\n",
    "## plot predictions\n",
    "plt.hist(loss_paired_aug, bins=20, range=[-1,1], density=True, facecolor='g', alpha=0.75, label='paired')\n",
    "plt.hist(loss_unpaired_aug, bins=20, range=[-1,1], density=True, facecolor='r', alpha=0.75, label='unpaired')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('/mnt/shared/celltracking/plots/klaus_noaugmentation_output12_epoch300_500samples.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56d9ed15-8548-442f-8f14-4eef382a5d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(modelstateP+\"/\"+stateFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f79674-368c-4d1d-8755-b212d8cfb952",
   "metadata": {},
   "source": [
    "# Function to compute model prediction at divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df081113-22b5-4907-a076-a6ff0397e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfromModel(coord_raw,coord_rawshift,volSize=(1,5,64, 64),paired=True):\n",
    "    # define gp pipeline\n",
    "    raw = gp.ArrayKey('raw')\n",
    "    raw = gp.ArrayKey('raw')\n",
    "    pipeline_allCentroids = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw_shift: 'raw', raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw_shift: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True), raw:gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)}  # meta-information\n",
    "    ) + gp.Pad(raw_shift, None) + gp.Pad(raw, None))\n",
    "    \n",
    "    # constructs gp pipeline\n",
    "    gp.ArraySpec()\n",
    "    request = gp.BatchRequest()\n",
    "    request[raw] = gp.Roi(coord_raw, volSize)\n",
    "    request[raw_shift] = gp.Roi(coord_rawshift, volSize)\n",
    "    \n",
    "    with gp.build(pipeline_allCentroids):\n",
    "        batch = pipeline_allCentroids.request_batch(request)\n",
    "\n",
    "    ## evaluate model for each centroid using gp pipeline\n",
    "    vol0 = batch[raw].data\n",
    "    vol1 = batch[raw_shift].data\n",
    "    vol0 = np.reshape(vol0, (volSize[0],volSize[2], volSize[3], volSize[1]))\n",
    "    vol1 = np.reshape(vol1, (volSize[0],volSize[2], volSize[3], volSize[1]))\n",
    "    vol0 = np.expand_dims(vol0, axis =0)\n",
    "    vol1 = np.expand_dims(vol1, axis =0)\n",
    "    \n",
    "    vol0 = torch.from_numpy(vol0).to(device).float()\n",
    "    vol1 = torch.from_numpy(vol1).to(device).float()\n",
    "    \n",
    "    # y for unpaired = -1 , y for paired = 1\n",
    "    if paired:\n",
    "        y = 1\n",
    "    else:\n",
    "        y =-1\n",
    "\n",
    "    y_torch = torch.from_numpy(np.array([y])).to(device).float()\n",
    "    pred0 = model(vol0)\n",
    "    pred1 = model(vol1)\n",
    "    \n",
    "    pred = torch.nn.functional.cosine_similarity(pred1,pred0).item()\n",
    "    #pred = np.float32(loss_contrastive.detach().cpu())    \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46a5f7-baba-4ae5-bf8e-36acd4aad0da",
   "metadata": {},
   "source": [
    "# Evaluate predictions at division points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07364cfd-eadc-4301-aae4-708e91f87ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " edge: ((1, 22), (2, 26)) returns loss: -0.06458476185798645\n",
      " edge: ((1, 22), (3, 26)) returns loss: -0.043957050889730453\n",
      " edge: ((4, 35), (5, 40)) returns loss: -0.0018070340156555176\n",
      " edge: ((4, 35), (6, 40)) returns loss: 0.3117785155773163\n",
      " edge: ((7, 69), (8, 75)) returns loss: 0.9948747754096985\n",
      " edge: ((7, 69), (9, 75)) returns loss: 0.9949910640716553\n"
     ]
    }
   ],
   "source": [
    "# for child_id, child_from, child_to, child_parent_id in links:\n",
    "#     for parent_id, _, parent_to, _ in links:\n",
    "#         if child_parent_id == parent_id:\n",
    "#             graph.add_edge((parent_id, parent_to), (child_id, child_from))\n",
    "            \n",
    "# edges = graph.edges\n",
    "# for edge in graph.edges():\n",
    "#      print(edge)\n",
    "\n",
    "#for track in tracks:\n",
    "\n",
    "\n",
    "for track in tracks:\n",
    "    for i,edge in enumerate(sorted(track.edges())):\n",
    "        #print(edge)\n",
    "        if edge[0][0] != edge[1][0]:\n",
    "            t0=graph.nodes[edge[0]]['t']\n",
    "            x0=graph.nodes[edge[0]]['x']\n",
    "            y0=graph.nodes[edge[0]]['y']\n",
    "            t1=graph.nodes[edge[1]]['t']\n",
    "            x1=graph.nodes[edge[1]]['x']\n",
    "            y1=graph.nodes[edge[1]]['y']\n",
    "\n",
    "            coord_raw = (t0,0,x0-(volSize[2]/2),y0-(volSize[3]/2))\n",
    "            coord_rawshift = (t1,0,x1-(volSize[2]/2),y1-(volSize[3]/2))\n",
    "            volSize = (1,5,64,64)\n",
    "            paired=True\n",
    "            \n",
    "            lossVal = lossfromModel(coord_raw,coord_rawshift,volSize,paired)\n",
    "            \n",
    "            print(f' edge: {edge} returns loss: {lossVal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae2018-a983-4245-9a43-0d1b94a6474a",
   "metadata": {},
   "source": [
    "# Predictions at division points for different model states\n",
    "### no augmentation, epoch 550\n",
    "* edge: ((1, 22), (2, 26)) returns loss: -0.19150486588478088\n",
    "* edge: ((1, 22), (3, 26)) returns loss: -0.1914677917957306\n",
    "* edge: ((4, 35), (5, 40)) returns loss: -0.2540801167488098\n",
    "* edge: ((4, 35), (6, 40)) returns loss: -0.48983266949653625\n",
    "* edge: ((7, 69), (8, 75)) returns loss: -0.009583104401826859\n",
    "* edge: ((7, 69), (9, 75)) returns loss: -0.04106420278549194\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
