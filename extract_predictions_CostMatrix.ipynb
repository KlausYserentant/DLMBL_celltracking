{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8785982-9655-49cb-8f12-e8f43dd02b19",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c484a69-ab7c-454c-9e8a-190ad19567e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llanosp/miniconda3/envs/celltracking/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gunpowder as gp\n",
    "import zarr\n",
    "import math\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import skimage\n",
    "import networkx\n",
    "import pathlib\n",
    "from tifffile import imread, imwrite\n",
    "import tensorboard\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8553a-bec0-4bb6-a756-98c407823208",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6816beac-5aa1-4c6f-b9e8-6c3c612813ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "input_size = (1, 64, 64, 5)\n",
    "downsample_factors =[(2, 2, 1), (2, 2, 1), (2, 2, 1), (2, 2, 1)];\n",
    "output_classes = 12\n",
    "\n",
    "# model definition\n",
    "class Vgg3D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_classes, downsample_factors, fmaps=12):\n",
    "\n",
    "        super(Vgg3D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.downsample_factors = downsample_factors\n",
    "        self.output_classes = 2\n",
    "\n",
    "        current_fmaps, h, w, d = tuple(input_size)\n",
    "        current_size = (h, w,d)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(downsample_factors)):\n",
    "\n",
    "            features += [\n",
    "                torch.nn.Conv3d(current_fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv3d(fmaps,fmaps,kernel_size=3,padding=1),\n",
    "                torch.nn.BatchNorm3d(fmaps),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.MaxPool3d(downsample_factors[i])\n",
    "            ]\n",
    "\n",
    "            current_fmaps = fmaps\n",
    "            fmaps *= 2\n",
    "\n",
    "            size = tuple(\n",
    "                int(c/d)\n",
    "                for c, d in zip(current_size, downsample_factors[i]))\n",
    "            check = (\n",
    "                s*d == c\n",
    "                for s, d, c in zip(size, downsample_factors[i], current_size))\n",
    "            assert all(check), \\\n",
    "                \"Can not downsample %s by chosen downsample factor\" % \\\n",
    "                (current_size,)\n",
    "            current_size = size\n",
    "\n",
    "        self.features = torch.nn.Sequential(*features)\n",
    "\n",
    "        classifier = [\n",
    "            torch.nn.Linear(current_size[0] *current_size[1]*current_size[2] *current_fmaps,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096,output_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, raw):\n",
    "\n",
    "        # add a channel dimension to raw\n",
    "        # shape = tuple(raw.shape)\n",
    "        # raw = raw.reshape(shape[0], 1, shape[1], shape[2])\n",
    "        \n",
    "        # compute features\n",
    "        f = self.features(raw)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        \n",
    "        # classify\n",
    "        y = self.classifier(f)\n",
    "\n",
    "        return y\n",
    "\n",
    "# create the model to train\n",
    "model = Vgg3D(input_size, output_classes,  downsample_factors = downsample_factors)\n",
    "model = model.to(device)\n",
    "#summary(model, input_size)\n",
    "\n",
    "# define loss function\n",
    "loss_function = torch.nn.CosineEmbeddingLoss()\n",
    "#loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec756973-b147-45fc-a919-a6baf16d4ff1",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f58afb48-ae4b-4f28-9b2f-1ed313b77963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg3D(\n",
       "  (features): Sequential(\n",
       "    (0): Conv3d(1, 12, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(12, 12, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv3d(12, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (8): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (11): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (15): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (18): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (22): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (25): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool3d(kernel_size=(2, 2, 1), stride=(2, 2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=7680, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=12, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to state file\n",
    "modelstateP = '/mnt/shared/celltracking/modelstates/aaron/'\n",
    "stateFile = 'epoch_27'\n",
    "\n",
    "model.load_state_dict(torch.load(modelstateP+stateFile))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55913bed-2a6a-448d-9a66-87875ecdd73d",
   "metadata": {},
   "source": [
    "# Extract cell- and frame-wise model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb9cc256-d559-4b6c-9946-e2bf3b4a631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with: 50/614 total\n",
      "done with: 100/614 total\n",
      "done with: 150/614 total\n",
      "done with: 200/614 total\n",
      "done with: 250/614 total\n",
      "done with: 300/614 total\n",
      "done with: 350/614 total\n",
      "done with: 400/614 total\n",
      "done with: 450/614 total\n",
      "done with: 500/614 total\n",
      "done with: 550/614 total\n",
      "done with: 600/614 total\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "volSize = (1,5,64, 64)\n",
    "zarrdir = '/mnt/shared/celltracking/data/cho/02.zarr'\n",
    "raw = gp.ArrayKey('raw')\n",
    "\n",
    "# extract centroids from annotated image stacks\n",
    "annotationPath = pathlib.Path(\"/mnt/shared/celltracking/data/cho/\")\n",
    "annotations = np.stack([imread(xi) for xi in sorted((annotationPath / \"02_GT/TRA\").glob(\"*.tif\"))])  # images\n",
    "cells = []\n",
    "for t, frame in enumerate(annotations):\n",
    "    centers = skimage.measure.regionprops(frame)\n",
    "    for c in centers:\n",
    "        cells.append([c.label, t, int(c.centroid[1]), int(c.centroid[2])])\n",
    "\n",
    "# define gp pipeline\n",
    "pipeline_allCentroids = (gp.ZarrSource(\n",
    "    zarrdir,  # the zarr container\n",
    "    {raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw: gp.ArraySpec(voxel_size=(1,1,1,1), interpolatable=True)})  # meta-information\n",
    "    + gp.Pad(raw, None))\n",
    "\n",
    "# constructs gp pipeline\n",
    "\n",
    "gp.ArraySpec()\n",
    "\n",
    "# loop over all cell centroids\n",
    "predictions = []\n",
    "i=0\n",
    "for id,t,x,y in cells:\n",
    "    # determine coordinates\n",
    "    coord = (t,0,x-(volSize[2]/2),y-(volSize[3]/2))\n",
    "    request = gp.BatchRequest()\n",
    "    request[raw] = gp.Roi(coord, volSize)\n",
    "    \n",
    "    with gp.build(pipeline_allCentroids):\n",
    "        batch = pipeline_allCentroids.request_batch(request)\n",
    "        \n",
    "    # show the content of the batch\n",
    "    # print(f\"batch returned: {batch}\")\n",
    "\n",
    "    # # plot first slice of volume\n",
    "    # print(batch[raw].data.shape)\n",
    "    # plt.imshow(np.flipud(batch[raw].data[0,0,:,:]))\n",
    "\n",
    "    ## evaluate model for each centroid using gp pipeline\n",
    "    vol = batch[raw].data\n",
    "    vol = np.reshape(vol, (1,64, 64, 5))\n",
    "    vol = np.expand_dims(vol, axis =0)\n",
    "    vol = torch.from_numpy(vol).to(device).float()\n",
    "    pred = model(vol)\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    \n",
    "    # save pred into list with id + position information\n",
    "    predictions.append([id, t, x, y, pred])\n",
    "    i += 1\n",
    "    if i%50==0:\n",
    "        print(f'done with: {i}/{len(cells)} total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1af85f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c42b0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6574/361194873.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tracks = np.array(predictions)[:,0:4]\n"
     ]
    }
   ],
   "source": [
    "tracks = np.array(predictions)[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daf21322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6574/2557424083.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(predictions)[:,4].shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(614,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions)[:,4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d5a9c",
   "metadata": {},
   "source": [
    "### MATRIX T and T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d9cdac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "distances = []\n",
    "#row corresponds to index of cells in t\n",
    "rows = [] \n",
    "#row corresponds to index of cells in t+1\n",
    "cols = []\n",
    "\n",
    "#Loop throug the times frames\n",
    "for t in range(max(tracks[:,1])):\n",
    "\n",
    "    #get index\n",
    "    idxt=np.where(tracks[:,1]==t)[0]\n",
    "    idxt_next=np.where(tracks[:,1]==t+1)[0]\n",
    "    t_matrix=np.zeros((len(idxt),len(idxt_next)), dtype=float)\n",
    "\n",
    "    for ii in range(0, len(idxt)):\n",
    "        for jj in range(0, len(idxt_next)):\n",
    "            #coordinate x,y cellN in t\n",
    "            pt1=[tracks[ii,2], tracks[ii,3]]\n",
    "            #pt1=tracks[ii,2:]-->embedding\n",
    "            #coordinate x,y cellN in t next\n",
    "            pt2=[tracks[jj,2], tracks[jj,3]] \n",
    "            #distance from pt1 and pt2\n",
    "            dist=distance.euclidean(pt1,pt2)\n",
    "            #dist = np.sqrt(np.sum(np.square(pt1-pt2)))\n",
    "            #fill matrix with the distances\n",
    "            \n",
    "            t_matrix[ii,jj]= dist\n",
    "            \n",
    "    #print(t, len(idxt), len(idxt_next))\n",
    "    distances.append(t_matrix)\n",
    "    rows.append(idxt)\n",
    "    cols.append(idxt_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b253b033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_em[43].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63566950",
   "metadata": {},
   "source": [
    "###  Embedding matrix distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62b3a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6574/3882608011.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  predictnp=np.array(predictions)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "predictnp=np.array(predictions)\n",
    "\n",
    "distances_em = []\n",
    "#row corresponds to index of cells in t\n",
    "rows_em = [] \n",
    "#row corresponds to index of cells in t+1\n",
    "cols_em = []\n",
    "\n",
    "#Loop throug the times frames\n",
    "for t in range(max(predictnp[:,1])):\n",
    "\n",
    "    #get index\n",
    "    idxt=np.where(predictnp[:,1]==t)[0]\n",
    "    idxt_next=np.where(predictnp[:,1]==t+1)[0]\n",
    "    t_matrix_emb=np.zeros((len(idxt),len(idxt_next)), dtype=float)\n",
    "\n",
    "    for ii in range(0, len(idxt)):\n",
    "        for jj in range(0, len(idxt_next)):\n",
    "            #coordinate x,y cellN in t\n",
    "            pt1=predictnp[ii,4][0]\n",
    "            #pt1=tracks[ii,2:]-->embedding\n",
    "            #coordinate x,y cellN in t next\n",
    "            pt2=predictnp[jj,4][0]\n",
    "            #distance from pt1 and pt2\n",
    "            dist=distance.euclidean(pt1,pt2)\n",
    "            #dist = np.sqrt(np.sum(np.square(pt1-pt2)))\n",
    "            #fill matrix with the distances\n",
    "            \n",
    "            t_matrix_emb[ii,jj]= dist\n",
    "            \n",
    "    #print(t, len(idxt), len(idxt_next))\n",
    "    distances_em.append(t_matrix_emb)\n",
    "    rows_em.append(idxt)\n",
    "    cols_em.append(idxt_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0766b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_matrix_t=distances[t] + distances_em[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a509a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 160, 297,\n",
       "        array([[  7770.195 , -18469.22  , -10354.515 ,  17674.717 ,   5795.583 ,\n",
       "                 16273.54  ,   7105.2046,  -4123.329 ,  -5032.6187, -16819.05  ,\n",
       "                  1590.4056,  10218.346 ]], dtype=float32)                      ],\n",
       "       [4, 0, 273, 208,\n",
       "        array([[  8536.617 , -21871.062 , -11443.676 ,  21673.814 ,   7095.0366,\n",
       "                 19508.102 ,   8196.135 ,  -4920.857 ,  -4863.902 , -20336.3   ,\n",
       "                  1596.6511,  12575.106 ]], dtype=float32)                      ],\n",
       "       [7, 0, 356, 272,\n",
       "        array([[  5733.0303 , -16769.85   ,  -4270.3696 ,  16714.781  ,\n",
       "                 -2707.229  ,   8084.8296 ,   9418.933  ,   -472.37906,\n",
       "                  4309.2476 , -22026.822  ,   5204.6724 ,  13365.835  ]],\n",
       "              dtype=float32)                                             ],\n",
       "       ...,\n",
       "       [20, 91, 99, 296,\n",
       "        array([[  6853.2026, -16915.326 ,  -8936.24  ,  17078.479 ,   5272.0557,\n",
       "                 15176.544 ,   6421.7646,  -3447.9907,  -3396.9817, -16076.896 ,\n",
       "                  1105.8854,  10177.395 ]], dtype=float32)                      ],\n",
       "       [21, 91, 389, 274,\n",
       "        array([[  7579.2944 , -21732.713  ,  -5817.614  ,  21191.398  ,\n",
       "                 -3530.889  ,  10294.272  ,  12282.667  ,   -778.01337,\n",
       "                  4971.7925 , -28221.303  ,   6963.1855 ,  16896.271  ]],\n",
       "              dtype=float32)                                             ],\n",
       "       [22, 91, 398, 99,\n",
       "        array([[  4138.66   , -11262.135  ,  -4203.2144 ,  11149.885  ,\n",
       "                   438.5166 ,   7366.258  ,   5522.507  ,  -1246.0068 ,\n",
       "                   559.94965, -12968.055  ,   2397.068  ,   7873.2485 ]],\n",
       "              dtype=float32)                                             ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f93231fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_two_frames(cost_matrix): \n",
    "    \n",
    "    cost_matrix = cost_matrix.copy()\n",
    "    b = self.birth_cost_factor * min(self.threshold, cost_matrix.max())\n",
    "    d = self.death_cost_factor * min(self.threshold, cost_matrix.max())\n",
    "    no_link = max(cost_matrix.max(), max(b, d)) * 1e9\n",
    "        \n",
    "    cost_matrix[cost_matrix > self.threshold] = no_link\n",
    "    lower_right = cost_matrix.transpose()\n",
    "\n",
    "    deaths = np.full(shape=(cost_matrix.shape[0], cost_matrix.shape[0]), fill_value=no_link)\n",
    "    np.fill_diagonal(deaths, d)\n",
    "    births = np.full(shape=(cost_matrix.shape[1], cost_matrix.shape[1]), fill_value=no_link)\n",
    "    np.fill_diagonal(births, b)\n",
    "        \n",
    "    square_cost_matrix = np.block([\n",
    "        [cost_matrix, deaths],\n",
    "        [births, lower_right],\n",
    "    ])\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(square_cost_matrix)\n",
    "        \n",
    "    ids_from = []\n",
    "    ids_to = []\n",
    "    births = []\n",
    "    deaths = []\n",
    "    for row, col in zip(row_ind, col_ind):\n",
    "        if row < cost_matrix.shape[0] and col < cost_matrix.shape[1]:\n",
    "            ids_from.append(row)\n",
    "            ids_to.append(col)\n",
    "\n",
    "        if row >= cost_matrix.shape[0] and col < cost_matrix.shape[1]:\n",
    "                births.append(col)\n",
    "        if row < cost_matrix.shape[0] and col >= cost_matrix.shape[1]:\n",
    "                deaths.append(row)\n",
    "\n",
    "    ids_from = np.array(ids_from)\n",
    "    ids_to = np.array(ids_to)\n",
    "    births = np.array(births)\n",
    "    deaths = np.array(deaths)\n",
    "                        \n",
    "    # Account for +1 offset of the dense labels\n",
    "    ids_from += 1\n",
    "    ids_to += 1\n",
    "    births += 1\n",
    "    deaths += 1\n",
    "        \n",
    "    links = {\"links\": (ids_from, ids_to), \"births\": births, \"deaths\": deaths}\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e64afc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(distances)):\n\u001b[1;32m      2\u001b[0m     cost_matrix_t\u001b[38;5;241m=\u001b[39mdistances[time] \u001b[38;5;241m+\u001b[39m distances_em[time]\n\u001b[0;32m----> 3\u001b[0m     links_data\u001b[38;5;241m=\u001b[39m\u001b[43mlink_two_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36mlink_two_frames\u001b[0;34m(cost_matrix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlink_two_frames\u001b[39m(cost_matrix): \n\u001b[1;32m      3\u001b[0m     cost_matrix \u001b[38;5;241m=\u001b[39m cost_matrix\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 4\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mbirth_cost_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, cost_matrix\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m      5\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeath_cost_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, cost_matrix\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m      6\u001b[0m     no_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cost_matrix\u001b[38;5;241m.\u001b[39mmax(), \u001b[38;5;28mmax\u001b[39m(b, d)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "for time in range(len(distances)):\n",
    "    cost_matrix_t=distances[time] + distances_em[time]\n",
    "    links_data=link_two_frames(cost_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd2d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
